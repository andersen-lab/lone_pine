import datetime

import geojson_rewind
import geopandas as gpd
import pandas as pd
import numpy as np
import json
import dashboard.plot as dashplot

cached_cases = None
cached_seqs = None

# Download metadata from SEARCH repository
# https://raw.githubusercontent.com/andersen-lab/HCoV-19-Genomics/master/metadata.csv
def download_search( window=None ):
    """ Downloads the metadata from the SEARCH github repository. Removes entries with very wrong dates.
    Parameters
    ----------
    window : int
        Number of days in the past to limit sequences to.

    Returns
    -------
    pandas.DataFrame:
        Data frame containing the metadata for all sequences generated by SEARCH
    """

    global cached_seqs
    if cached_seqs is not None:
        print( "Found cached version of sequences. Loading locally..." )
        md = cached_seqs
    else:
        print( "Unable to find cached version of sequences. Downloading from HCoV-19-Genomics repository." )
        search_md = "https://raw.githubusercontent.com/andersen-lab/HCoV-19-Genomics/master/metadata.csv"
        md = pd.read_csv( search_md )
        drop_cols = [i for i in md.columns if i not in ["ID", "collection_date", "location", "authors", "originating_lab", "zipcode"]]
        md = md.drop( columns=drop_cols )
        md = md.loc[md["collection_date"]!='Unknown']
        md = md.loc[~md["collection_date"].str.startswith( "19" )]
        md = md.loc[~md["collection_date"].str.contains( "/" )]
        md["collection_date"] = pd.to_datetime( md["collection_date"], format="%Y-%m-%d" ).dt.normalize()
        md["days_past"] = ( datetime.datetime.today() - md["collection_date"] ).dt.days
        cached_seqs = md

    if window is not None:
        md = md.loc[md["days_past"] <= window].copy()

    md = md.drop( columns=["days_past"] )
    return md

def download_shapefile( cases, seqs, save=False, local=False ):
    """ Downloads and formats the San Diego ZIP GeoJSON formatted as a dictionary.
    Parameters
    ----------
    cases : pandas.DataFrame
        output of download_cases() containing the cummulative cases for each ZIP code.
    seqs : pandas.DataFrame
        output of download_search() containing a list of sequences with ZIP code information.
    save : bool
        specifies whether to save the generated shapefile. Internal use only.
    local : bool
        specifies whether to load shapefile from local resources, or to re-download from arcgis.
    Returns
    -------
    geopandas.GeoDataFrame:
        GeoDataFrame linking ZIP code areas to case counts, sequences, and fraction of cases sequenced.
    """
    if local:
        zip_area = gpd.read_file( "resources/zips.geojson")
    else:
        shapefile_loc = "https://opendata.arcgis.com/datasets/41c3a7bd375547069a78fce90153cbc0_5.geojson"
        zip_area = gpd.read_file( shapefile_loc )
        zip_area = zip_area[["ZIP", "geometry"]].dissolve( by="ZIP" )
        zip_area = zip_area.reset_index()

        # GeoJSON from San Diego has improper winding so I have to fix it.
        zip_area = zip_area.set_geometry(
            gpd.GeoDataFrame.from_features(
                json.loads(
                    geojson_rewind.rewind(
                        zip_area.to_json(),
                        rfc7946=False
                    )
                )["features"]
            ).geometry
        )

    if save:
        zip_area.to_file("resources/zips.geojson", driver='GeoJSON' )

    # Add case data so it is there...
    zip_area = zip_area.merge( cases, left_on="ZIP", right_on="ziptext" )
    zip_area = zip_area.merge( get_seqs( seqs, groupby="zipcode" ), left_on="ZIP", right_on="zip", how="left" )
    zip_area["sequences"] = zip_area["sequences"].fillna( 0 )
    zip_area["fraction"] = zip_area["sequences"] / zip_area["case_count"]
    zip_area = zip_area.set_index( "ZIP" )

    # Removing a number of columns to save memory.
    zip_area = zip_area[["geometry", "case_count","sequences", "fraction"]]

    return zip_area

# Grab covid statistics from data repository
# https://gis-public.sandiegocounty.gov/arcgis/rest/services/Hosted/COVID_19_Statistics__by_ZIP_Code/FeatureServer/0/query?outFields=*&where=1%3D1
def download_cases( window=None):
    """ Downloads the cases per San Diego ZIP code. Appends population and ZIP code shapefile.
    Parameters
    ----------
    window : int
        Number of days in the past to limit cases to.
    Returns
    -------
    pandas.DataFrame
        DataFrame detailing the cummulative cases in each ZIP code.
    """
    def _append_population( dataframe ):
        pop_loc = "resources/zip_pop.csv"
        pop = pd.read_csv( pop_loc )
        pop = pop.set_index( "Zip" )
        pop["Total Population"] = pd.to_numeric( pop["Total Population"].str.replace( ",", "" ) )
        pop = pop["Total Population"].to_dict()
        dataframe["population"] = dataframe["ziptext"].map( pop )
        return dataframe

    global cached_cases
    if cached_cases is not None:
        print( "Found cached version of cases. Loading locally..." )
        return_df = cached_cases
    else:
        print( "Unable to find cached version of cases. Downloading from arcigs..." )
        cases_loc = "https://opendata.arcgis.com/datasets/854d7e48e3dc451aa93b9daf82789089_0.geojson"
        return_df = gpd.read_file( cases_loc )
        return_df = return_df[["ziptext","case_count", "updatedate"]]
        return_df["updatedate"] = pd.to_datetime( return_df["updatedate"] ).dt.tz_localize( None )
        return_df["updatedate"] = return_df["updatedate"].dt.normalize()
        return_df["ziptext"] = pd.to_numeric( return_df["ziptext"] )
        return_df = return_df.groupby( ["updatedate", "ziptext"] ).first().reset_index()
        return_df = _append_population( return_df )
        return_df = return_df.sort_values( "updatedate" )

        # Calculate cases per day because thats way more useable than cummulative counts.
        return_df = _append_population( return_df )
        return_df["case_count"] = return_df["case_count"].fillna( 0 )
        return_df["new_cases"] = return_df.groupby( "ziptext" )["case_count"].diff()
        return_df["new_cases"] = return_df["new_cases"].fillna( return_df["case_count"] )
        return_df.loc[return_df["new_cases"]<0, "new_cases"] = 0

        return_df["days_past"] = ( datetime.datetime.today() - return_df["updatedate"] ).dt.days

        # Save the dataframe so that we don't need to download again.
        cached_cases = return_df

    if window is not None:
        return_df = return_df.loc[return_df["days_past"] <= window].copy()

    return_df["case_count"] = return_df.groupby( "ziptext" )["new_cases"].cumsum()

    ts_df = return_df.melt( id_vars=["updatedate", "ziptext"], value_vars=['case_count'] )

    # For one dataset only keep the most recent data.
    return_df = return_df.sort_values( "updatedate", ascending=False ).groupby( "ziptext" ).first()
    return_df = return_df.reset_index()
    return_df = return_df.drop( columns=["days_past"] )

    return return_df, ts_df

def get_seqs( seq_md, groupby="collection_date", zip_f=None ):
    """ Pivots the output of download_search().
    Parameters
    ----------
    seq_md : pandas.DataFrame
        output of download_search(); list of sequences attached to ZIP code and collection date.
    groupby : str
        column of seq_md to count.
    zip_f : bool
        indicates whether to filter sequences to a single zipcode.

    Returns
    -------
    pandas.DatFrame
    """
    if zip_f:
        seqs = seq_md.loc[seq_md["zipcode"].isin( zip_f )]
    else:
        seqs = seq_md

    seqs = seqs.groupby( groupby )["ID"].agg( "count" ).reset_index()
    if groupby == "collection_date":
        seqs.columns = ["date", "new_sequences"]
    elif groupby == "zipcode":
        seqs.columns = ["zip", "sequences"]

    return seqs

def get_seqs_per_case( time_series, seq_md, zip_f=None, normalized=False ):
    """ Combines timeseries of
    Parameters
    ----------
    time_series
    seq_md
    zip_f
    normalized

    Returns
    -------

    """
    field = "case_100k" if normalized else "case_count"

    query = time_series["variable"]==field
    if zip_f:
        if type( zip_f ) != list:
            zip_f = [zip_f]
        query = query & ( time_series["ziptext"].isin( zip_f ) )
    cases = time_series.loc[query].pivot_table( index="updatedate", values="value", aggfunc="sum" )
    cases = cases.fillna( 0.0 )
    cases = cases.reset_index()

    cases.columns = ["date", "cases"]

    #cases = cases.merge( get_seqs( seq_md ), on="date", how="outer", sort=True )

    try:
        cases = cases.merge( get_seqs( seq_md, zip_f=zip_f ), on="date", how="outer", sort=True )
    except ValueError:
        print( cases.head() )
        print( get_seqs( seq_md ).head() )
        print( cases.dtypes )
        print( get_seqs( seq_md ).dtypes )
        exit( 1 )


    cases["new_sequences"] = cases["new_sequences"].fillna( 0.0 )
    cases["sequences"] = cases["new_sequences"].cumsum()
    #cases.to_csv( "resources/temp.csv" )
    cases = cases.loc[~cases["cases"].isna()]
    cases["new_cases"] = cases["cases"].diff()
    cases["new_cases"] = cases["new_cases"].fillna( 0.0 )
    cases.loc[cases["new_cases"] < 0,"new_cases"] = 0
    cases["fraction"] = cases["sequences"] / cases["cases"]

    return cases


if __name__ == "__main__":
    download_search( window=1000 )

#    md = pd.read_csv( "resources/md.csv" )
#    md = md.loc[md["collection_date"]!='Unknown']
#    md = md.loc[~md["collection_date"].str.startswith( "19" )]
#    md = md.loc[~md["collection_date"].str.contains( "/" )]
#    md["collection_date"] = pd.to_datetime( md["collection_date"], format="%Y-%m-%d" ).dt.normalize()
#    df, ts = download_cases()
#    zips = download_shapefile( df, md, local=True )
#    plot_df = get_seqs_per_case( ts, md )
#
#    fig = dashplot.plot_choropleth( zips )
#    fig.show()
#
#    #fig = dashplot.plot_daily_cases_seqs( plot_df )
#    #fig.show()